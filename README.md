MNA_MLOps_Equipo12
==============================

Project created for the MLOps assignature in MNA-V Tec de Monterrey

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

Aplicaci√≥n de Predicci√≥n de Seguros de Caravanas

Este README explica los pasos necesarios para configurar, ejecutar el pipeline de datos, y desplegar la aplicaci√≥n usando Docker.

--------------------------------------------------------------------------------

1. Ejecuci√≥n del Pipeline de Procesamiento de Datos

El pipeline se encarga de leer los datos crudos, ejecutar todo el procesamiento estructurado y guardar el modelo necesario en la ubicaci√≥n requerida por la aplicaci√≥n.

Instrucciones

Para preparar el modelo y dejarlo en la ruta correcta (src/pipeline), ejecuta el siguiente archivo:

python src/pipelines/main_pipeline.py

* Funci√≥n: Este script lee la data raw (df original), ejecuta toda la programaci√≥n estructurada del flujo de trabajo de Machine Learning, y finaliza guardando el archivo del pipeline en la carpeta src.
* Importante: El archivo del pipeline debe existir en la ruta src para que la aplicaci√≥n pueda levantarse y cargarlo correctamente para realizar las predicciones.

--------------------------------------------------------------------------------

2. Despliegue de la Aplicaci√≥n con Docker

Para desplegar la aplicaci√≥n como un servicio, usaremos Docker.

Requisitos

* Tener Docker instalado en tu sistema.

Pasos para el Despliegue

A. Crear el archivo de Variables de Entorno (.env)

Antes de construir la imagen, aseg√∫rate de crear y configurar un archivo llamado .env en la ra√≠z del proyecto. Este archivo debe contener, como m√≠nimo, la siguiente configuraci√≥n:

API_WORKERS=1

B. Construir la Imagen de Docker

Ejecuta el siguiente comando en la terminal desde la ra√≠z del proyecto para construir la imagen.

sudo docker build -t caravan_predictions .

* Etiqueta: La imagen se etiquetar√° como caravan_predictions.

C. Ejecutar el Contenedor de Docker

Una vez que la imagen est√© construida, ejecuta el siguiente comando para levantar la aplicaci√≥n.

docker run -p 8080:8080 --env-file .env caravan_predictions

* Mapeo de Puertos: Mapea el puerto 8080 del host al puerto 8080 del contenedor.
* Variables de Entorno: Carga las variables definidas en el archivo .env.

--------------------------------------------------------------------------------

# üöÄ Instrucciones para desplegar **caravan_prediction:v1.x.x**

## üê≥ 1. Descargar desde Docker Hub (si est√° disponible online)
```bash
docker pull caravan_prediction:v1.x.x
docker run -p 8080:8080 --env-file .env caravan_prediction:v1.x.x
```

> üí° *Aseg√∫rate de reemplazar `v1.x.x` por la versi√≥n m√°s reciente publicada.*

---

## üõ†Ô∏è 2. Construir la imagen desde el repositorio local
Si prefieres compilar la imagen t√∫ mismo, utiliza los siguientes comandos:

```bash
# Construir la imagen
docker build -t caravan_prediction:v1.0.0 .

# Ejecutar la aplicaci√≥n en el puerto 8080 usando el archivo .env
docker run -p 8080:8080 --env-file .env caravan_prediction:v1.0.0
```

> ‚öôÔ∏è Esto levantar√° el servicio FastAPI en http://localhost:8080

---

## üîê 3. Archivo `.env` ‚Äî Variables necesarias

Tu archivo `.env` debe incluir las siguientes variables de entorno, esenciales para la conexi√≥n con AWS S3 y la configuraci√≥n del API:

| Variable | Descripci√≥n |
|-----------|--------------|
| **AWS_DEFAULT_REGION** | Regi√≥n de AWS donde se aloja el bucket (ejemplo: `us-east-2`). |
| **AWS_ACCESS_KEY_ID** | ID de la clave de acceso para autenticaci√≥n en AWS. |
| **AWS_SECRET_ACCESS_KEY** | Clave secreta asociada al `AWS_ACCESS_KEY_ID`. |
| **S3_BUCKET_NAME** | Nombre del bucket en S3 donde se almacenan los modelos y pipelines (por defecto `mna-tec-mlops`). |
| **S3_MODEL_KEY** | Ruta completa del modelo dentro del bucket (por ejemplo `h2o_models/models/GBM_3_AutoML_1_20251112_191136`). |
| **API_WORKERS** | N√∫mero de workers de Uvicorn que manejar√°n las peticiones del API (por defecto `1`). |
| **BEST_THRESHOLD** | Umbral de decisi√≥n para el clasificador H2O (por defecto `0.1`). |

---

## üß© 4. Ejemplo de archivo `.env`
```env
AWS_DEFAULT_REGION=us-east-2
AWS_ACCESS_KEY_ID=TU_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=TU_SECRET_KEY
S3_BUCKET_NAME=mna-tec-mlops
S3_MODEL_KEY=h2o_models/models/GBM_3_AutoML_1_20251112_191136
API_WORKERS=1
BEST_THRESHOLD=0.1
S3_MODEL_PATH=h2o_models/models
```

---

## ‚úÖ 5. Verificaci√≥n del despliegue
Una vez el contenedor est√© en ejecuci√≥n, abre tu navegador o usa `curl`:

```bash
curl http://localhost:8080/docs
```

Esto deber√≠a mostrar la documentaci√≥n interactiva del API (**Swagger UI**) para el servicio de predicci√≥n `caravan_prediction`.

--------------------------------------------------------------------------------

3. Uso del Endpoint de Predicci√≥n

Una vez que la aplicaci√≥n est√© levantada y el contenedor de Docker est√© ejecut√°ndose, puedes utilizar el siguiente endpoint para enviar un archivo CSV y recibir las predicciones.

Endpoint de Predicci√≥n

Utiliza el siguiente comando curl para interactuar con la API:

curl --location 'http://localhost:8080/api/caravan-prediction/predict' \
--form 'file=@"{local_path}/MNA_MLOps_Equipo12_backup/tests/data_tests/prueba_mlops.csv"'

* Reemplazar {local_path}: Aseg√∫rate de sustituir {local_path} con la ruta absoluta de tu m√°quina donde se encuentra el archivo de prueba (e.g., /home/user/my_project).
* M√©todo: POST (impl√≠cito con --form).
* Formato de Datos: El endpoint espera un archivo CSV subido a trav√©s de un form-data con la clave file.

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
