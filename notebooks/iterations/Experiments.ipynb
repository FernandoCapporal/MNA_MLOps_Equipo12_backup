{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Librerías ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "import os\n",
    "\n",
    "# Configuración de gráficos\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdefed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Configuración del logger\n",
    "logger = logging.getLogger('preprocessor_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s'\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df: pd.DataFrame, target_col: str = 'target'):\n",
    "        \"\"\"\n",
    "        Inicializa el preprocesador con el DataFrame original.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame a procesar\n",
    "            target_col: Nombre de la columna target\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.original_df = df.copy()\n",
    "        self.processed_df = None\n",
    "        self.cor_target = None\n",
    "        self.product_cols = None\n",
    "        \n",
    "        # Atributos para exportar\n",
    "        self.zones_ = None\n",
    "        self.zone_mapper_df = None\n",
    "        self.cols_to_drop_ = []\n",
    "        self.power_params_ = None\n",
    "        self.skewed_cols_ = []\n",
    "        self.high_corr_df_products = None\n",
    "        \n",
    "        logger.info(f\"Preprocessor inicializado - Shape original: {self.original_df.shape}\")\n",
    "\n",
    "    def _log_shape_change(self, previous_shape: tuple, operation: str):\n",
    "        \"\"\"Registra cambios en el shape del DataFrame.\"\"\"\n",
    "        current_shape = self.processed_df.shape\n",
    "        logger.info(\n",
    "            f\"{operation} - Shape cambiado de {previous_shape} a {current_shape} \"\n",
    "            f\"(Filas: {previous_shape[0]} → {current_shape[0]}, \"\n",
    "            f\"Columnas: {previous_shape[1]} → {current_shape[1]})\"\n",
    "        )\n",
    "\n",
    "    def group_sociodemographic_cols(self, sociodemographic_cols: list):\n",
    "        \"\"\"\n",
    "        Agrupa columnas sociodemográficas en una sola columna 'zone'.\n",
    "        \n",
    "        Args:\n",
    "            sociodemographic_cols: Lista de columnas sociodemográficas a agrupar\n",
    "        \"\"\"\n",
    "        logger.info(f\"Iniciando agrupación de columnas sociodemográficas: {sociodemographic_cols}\")\n",
    "        \n",
    "        previous_shape = self.original_df.shape\n",
    "        \n",
    "        # Crear columna zone basada en agrupación\n",
    "        self.original_df['zone'] = self.original_df.groupby(sociodemographic_cols).ngroup() + 1\n",
    "        \n",
    "        # Crear mapper de zonas\n",
    "        expanded_cols = sociodemographic_cols + ['zone']\n",
    "        self.zone_mapper_df = self.original_df[expanded_cols].drop_duplicates()\n",
    "        \n",
    "        self.zones_ = [str(col) for col in self.zone_mapper_df['zone'].unique()]\n",
    "        \n",
    "        # Eliminar columnas originales\n",
    "        self.original_df.drop(columns=sociodemographic_cols, inplace=True)\n",
    "        self.processed_df = self.original_df.copy()\n",
    "        \n",
    "        logger.info(f\"Agrupación completada - {len(self.zone_mapper_df)} zonas únicas creadas\")\n",
    "        self._log_shape_change(previous_shape, \"Agrupación sociodemográfica\")\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Elimina duplicados exactos del DataFrame.\"\"\"\n",
    "        logger.info(\"Iniciando eliminación de duplicados exactos\")\n",
    "        \n",
    "        previous_shape = self.processed_df.shape\n",
    "        initial_rows = len(self.processed_df)\n",
    "        \n",
    "        self.processed_df = self.processed_df.drop_duplicates()\n",
    "        final_rows = len(self.processed_df)\n",
    "        removed_rows = initial_rows - final_rows\n",
    "        \n",
    "        logger.info(f\"Duplicados eliminados: {removed_rows} filas removidas\")\n",
    "        self._log_shape_change(previous_shape, \"Eliminación de duplicados exactos\")\n",
    "\n",
    "    def handle_complex_duplicates(self):\n",
    "        \"\"\"\n",
    "        Maneja duplicados complejos donde las filas son idénticas excepto por el target.\n",
    "        Conserva la moda del target en casos de múltiples duplicados.\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando manejo de duplicados complejos\")\n",
    "        \n",
    "        df = self.processed_df.copy()\n",
    "        previous_shape = df.shape\n",
    "        initial_rows = len(df)\n",
    "        \n",
    "        # Agrupar filas duplicadas excepto por target\n",
    "        columns = list(set(df.columns) - {self.target_col})\n",
    "        grupos_duplicados = df.groupby(columns).groups\n",
    "        \n",
    "        indices_a_eliminar = []\n",
    "        grupos_procesados = 0\n",
    "        \n",
    "        for fila, indices in grupos_duplicados.items():\n",
    "            if len(indices) > 1:  # Solo grupos con duplicados\n",
    "                grupos_procesados += 1\n",
    "                grupo_actual = df.loc[indices]\n",
    "                \n",
    "                if len(indices) == 2:\n",
    "                    # Eliminar todo el grupo (ambas filas)\n",
    "                    indices_a_eliminar.extend(indices)\n",
    "                else:\n",
    "                    # Grupos con más de 2 filas: conservar solo la moda\n",
    "                    moda_target = grupo_actual[self.target_col].mode()\n",
    "                    \n",
    "                    if len(moda_target) > 0:\n",
    "                        moda = moda_target[0]\n",
    "                        # Conservar solo las filas con target = moda\n",
    "                        filas_a_eliminar = grupo_actual[grupo_actual[self.target_col] != moda].index\n",
    "                        indices_a_eliminar.extend(filas_a_eliminar)\n",
    "                    else:\n",
    "                        # Si no hay moda clara, eliminar todo el grupo\n",
    "                        indices_a_eliminar.extend(indices)\n",
    "        \n",
    "        # Crear nuevo DataFrame sin los duplicados problemáticos\n",
    "        self.processed_df = df.drop(indices_a_eliminar)\n",
    "        final_rows = len(self.processed_df)\n",
    "        removed_rows = initial_rows - final_rows\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Manejo de duplicados complejos completado - \"\n",
    "            f\"{grupos_procesados} grupos procesados, {removed_rows} filas eliminadas\"\n",
    "        )\n",
    "        self._log_shape_change(previous_shape, \"Manejo de duplicados complejos\")\n",
    "\n",
    "    def get_correlations(self):\n",
    "        \"\"\"Calcula correlaciones punto-biserial entre features y target.\"\"\"\n",
    "        logger.info(\"Calculando correlaciones con el target\")\n",
    "        \n",
    "        if self.target_col not in self.processed_df.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_col}' no encontrada en el DataFrame\")\n",
    "        \n",
    "        cor_target = {}\n",
    "        features = [col for col in self.processed_df.columns if col != self.target_col]\n",
    "\n",
    "        for col in features:\n",
    "            corr, _ = pointbiserialr(self.processed_df[col], self.processed_df[self.target_col])\n",
    "            cor_target[col] = corr\n",
    "\n",
    "        self.cor_target = pd.Series(cor_target).sort_values(key=abs, ascending=False)\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Correlaciones calculadas - \"\n",
    "            f\"Rango: [{self.cor_target.min():.3f}, {self.cor_target.max():.3f}], \"\n",
    "            f\"Top 3: {self.cor_target.head(3).to_dict()}\"\n",
    "        )\n",
    "\n",
    "    def get_high_pair_correlations(self, product_cols: list):\n",
    "        \"\"\"\n",
    "        Identifica pares de variables con alta correlación entre sí.\n",
    "        \n",
    "        Args:\n",
    "            product_cols: Lista de columnas de productos a analizar\n",
    "        \"\"\"\n",
    "        logger.info(f\"Buscando correlaciones altas entre {len(product_cols)} columnas de productos\")\n",
    "        \n",
    "        # Verificar que las columnas existen\n",
    "        missing_cols = set(product_cols) - set(self.processed_df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Columnas no encontradas: {missing_cols}\")\n",
    "        \n",
    "        corr_abs_products = self.processed_df[product_cols].corr().abs()\n",
    "\n",
    "        high_corr_pairs_products = []\n",
    "        for i in range(len(corr_abs_products.columns)):\n",
    "            for j in range(i+1, len(corr_abs_products.columns)):\n",
    "                correlation = corr_abs_products.iloc[i, j]\n",
    "                if correlation > 0.7:\n",
    "                    high_corr_pairs_products.append({\n",
    "                        'var1': corr_abs_products.columns[i],\n",
    "                        'var2': corr_abs_products.columns[j], \n",
    "                        'correlation': correlation\n",
    "                    })\n",
    "\n",
    "        self.high_corr_df_products = pd.DataFrame(high_corr_pairs_products).sort_values(\n",
    "            'correlation', ascending=False\n",
    "        )\n",
    "        \n",
    "        # Filtrar correlaciones muy altas\n",
    "        high_corr_count = len(self.high_corr_df_products[self.high_corr_df_products['correlation'] > 0.95])\n",
    "        self.product_cols = product_cols\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Análisis de correlación completado - \"\n",
    "            f\"{len(high_corr_pairs_products)} pares con correlación > 0.7, \"\n",
    "            f\"{high_corr_count} pares con correlación > 0.95\"\n",
    "        )\n",
    "\n",
    "    def drop_high_correlated_cols(self):\n",
    "        \"\"\"Elimina columnas altamente correlacionadas, conservando las más relevantes.\"\"\"\n",
    "        logger.info(\"Iniciando eliminación de columnas altamente correlacionadas\")\n",
    "        \n",
    "        if self.high_corr_df_products is None:\n",
    "            raise ValueError(\"Debe ejecutar get_high_pair_correlations primero\")\n",
    "        \n",
    "        previous_shape = self.processed_df.shape\n",
    "        initial_cols = len(self.processed_df.columns)\n",
    "        \n",
    "        high_corr_filtered = self.high_corr_df_products[self.high_corr_df_products['correlation'] > 0.95]\n",
    "        \n",
    "        for _, row in high_corr_filtered.iterrows():\n",
    "            var1 = row['var1']\n",
    "            var2 = row['var2']\n",
    "\n",
    "            # Comparar la correlación absoluta con el target\n",
    "            corr_var1 = abs(self.cor_target[var1])\n",
    "            corr_var2 = abs(self.cor_target[var2])\n",
    "\n",
    "            # Quedarse con la columna más correlacionada, eliminar la otra\n",
    "            if corr_var1 < corr_var2:\n",
    "                col_to_drop = var1\n",
    "                col_to_keep = var2\n",
    "            else:\n",
    "                col_to_drop = var2\n",
    "                col_to_keep = var1\n",
    "            \n",
    "            if col_to_drop not in self.cols_to_drop_:\n",
    "                self.cols_to_drop_.append(col_to_drop)\n",
    "                logger.debug(f\"Marcada para eliminar: {col_to_drop} (corr: {corr_var1:.3f}) \"\n",
    "                           f\"vs {col_to_keep} (corr: {corr_var2:.3f})\")\n",
    "\n",
    "        self.cols_to_drop_ = list(set(self.cols_to_drop_))\n",
    "        self.product_cols = list(set(product_cols)-set(self.cols_to_drop_))\n",
    "        \n",
    "        # Eliminar columnas\n",
    "        columns_before_drop = set(self.processed_df.columns)\n",
    "        self.processed_df = self.processed_df.drop(columns=self.cols_to_drop_)\n",
    "        columns_after_drop = set(self.processed_df.columns)\n",
    "        dropped_columns = columns_before_drop - columns_after_drop\n",
    "        \n",
    "        final_cols = len(self.processed_df.columns)\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Eliminación de columnas correlacionadas completada - \"\n",
    "            f\"{len(dropped_columns)} columnas eliminadas: {list(dropped_columns)}\"\n",
    "        )\n",
    "        self._log_shape_change(previous_shape, \"Eliminación de columnas correlacionadas\")\n",
    "\n",
    "    def correct_skewness(self):\n",
    "        \"\"\"\n",
    "        Corrige asimetría en las columnas usando transformación Yeo-Johnson.\n",
    "        \n",
    "        Args:\n",
    "            product_cols: Lista de columnas de productos a transformar\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando corrección de asimetría\")\n",
    "        \n",
    "        # Calcular asimetría inicial\n",
    "        skewness_before = self.processed_df[self.product_cols].skew()\n",
    "        self.skewed_cols_ = skewness_before[abs(skewness_before) > 0.5].index.tolist()\n",
    "        \n",
    "        if not self.skewed_cols_:\n",
    "            logger.info(\"No se encontraron columnas con asimetría significativa (> 0.5)\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"{len(self.skewed_cols_)} columnas con asimetría > 0.5: {self.skewed_cols_}\")\n",
    "        \n",
    "        # Aplicar transformación Yeo-Johnson\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        self.processed_df[self.skewed_cols_] = pt.fit_transform(self.processed_df[self.skewed_cols_])\n",
    "        \n",
    "        # Calcular asimetría después de la transformación\n",
    "        skewness_after = self.processed_df[self.skewed_cols_].skew()\n",
    "        \n",
    "        # Guardar parámetros\n",
    "        if hasattr(pt, \"lambdas_\"):\n",
    "            self.power_params_ = dict(zip(self.skewed_cols_, pt.lambdas_))\n",
    "        else:\n",
    "            self.power_params_ = {}\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Corrección de asimetría completada - \"\n",
    "            f\"Asimetría promedio: {skewness_before.mean():.3f} → {skewness_after.mean():.3f}\"\n",
    "        )\n",
    "\n",
    "    def apply_one_hot(self):\n",
    "        \"\"\"Aplica one-hot encoding a la columna 'zone'.\"\"\"\n",
    "        logger.info(\"Aplicando one-hot encoding a la columna 'zone'\")\n",
    "        \n",
    "        if 'zone' not in self.processed_df.columns:\n",
    "            raise ValueError(\"Columna 'zone' no encontrada para one-hot encoding\")\n",
    "        \n",
    "        previous_shape = self.processed_df.shape\n",
    "        \n",
    "        # Convertir a entero y aplicar one-hot\n",
    "        self.processed_df['zone'] = self.processed_df['zone'].astype(int)\n",
    "        zone_dummies = pd.get_dummies(self.processed_df['zone'], prefix='zone')\n",
    "        \n",
    "        # Concatenar y eliminar columna original\n",
    "        self.processed_df = pd.concat([\n",
    "            self.processed_df.drop('zone', axis=1), \n",
    "            zone_dummies\n",
    "        ], axis=1)\n",
    "        \n",
    "        logger.info(f\"One-hot encoding completado - {len(zone_dummies.columns)} columnas zone creadas\")\n",
    "        self._log_shape_change(previous_shape, \"One-hot encoding\")\n",
    "\n",
    "    def apply_preprocess(self, sociodemographic_cols: list, product_cols: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ejecuta el pipeline completo de preprocesamiento.\n",
    "        \n",
    "        Args:\n",
    "            sociodemographic_cols: Columnas sociodemográficas a agrupar\n",
    "            product_cols: Columnas de productos para análisis de correlación\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame procesado\n",
    "        \"\"\"\n",
    "        logger.info(\"=== INICIANDO PIPELINE COMPLETO DE PREPROCESAMIENTO ===\")\n",
    "        logger.info(f\"Columnas sociodemográficas: {sociodemographic_cols}\")\n",
    "        logger.info(f\"Columnas de productos: {product_cols}\")\n",
    "        \n",
    "        # Pipeline de procesamiento\n",
    "        self.group_sociodemographic_cols(sociodemographic_cols)\n",
    "        self.remove_duplicates()\n",
    "        self.handle_complex_duplicates()\n",
    "        self.get_correlations()\n",
    "        self.get_high_pair_correlations(product_cols)\n",
    "        self.drop_high_correlated_cols()\n",
    "        self.correct_skewness()\n",
    "        self.apply_one_hot()\n",
    "        \n",
    "        logger.info(\"=== PIPELINE COMPLETADO EXITOSAMENTE ===\")\n",
    "        logger.info(f\"Shape final del DataFrame: {self.processed_df.shape}\")\n",
    "        logger.info(f\"Columnas eliminadas: {len(self.cols_to_drop_)}\")\n",
    "        logger.info(f\"Columnas transformadas: {len(self.skewed_cols_)}\")\n",
    "        \n",
    "        return self.processed_df\n",
    "\n",
    "    def get_preprocessing_summary(self) -> dict:\n",
    "        \"\"\"Retorna un resumen del proceso de preprocesamiento.\"\"\"\n",
    "        return {\n",
    "            'original_shape': self.original_df.shape,\n",
    "            'processed_shape': self.processed_df.shape if self.processed_df is not None else None,\n",
    "            'zones_created': len(self.zone_mapper_df) if self.zone_mapper_df is not None else 0,\n",
    "            'columns_dropped': len(self.cols_to_drop_),\n",
    "            'columns_skewness_corrected': len(self.skewed_cols_),\n",
    "            'high_correlation_pairs': len(self.high_corr_df_products) if self.high_corr_df_products is not None else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d633b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luisfernandocaporalmontesdeoca/Documents/MNA-V Tec de Monterrey/MLOps/MNA_MLOps_Equipo12_backup\r\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../..\")  # subir al nivel raíz del proyecto\n",
    "!pwd  # verificar ruta actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7ebc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting                                            |0.00 [00:00,    ?entry/s]\n",
      "Fetching\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/luisfernandocaporalmontesdeoca/Documents/MNA-V Te\u001b[A\n",
      "Fetching                                                                        \u001b[A\n",
      "Building workspace index                              |3.00 [00:00,  431entry/s]\n",
      "Comparing indexes                                    |4.00 [00:00, 1.38kentry/s]\n",
      "Applying changes                                      |0.00 [00:00,     ?file/s]\n",
      "Everything is up to date.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc pull data/raw/insurance_company_original.csv.dvc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"notebooks\")\n",
    "\n",
    "\n",
    "file_name = '../data/raw/insurance_company_original.csv'\n",
    "sociodemographic_cols = [f\"SD_{i}\" for i in range(1, 44)]\n",
    "product_cols = [f\"PD_{i-44}\" for i in range(44, 86)]\n",
    "cols = sociodemographic_cols + product_cols + [\"target\"]\n",
    "df = pd.read_csv(file_name, header=None, names=cols)\n",
    "df = df.iloc[1:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806fa7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 16:54:14,797 - preprocessor_logger - INFO - [__init__] - Preprocessor inicializado - Shape original: (5821, 86)\n",
      "2025-11-11 16:54:14,798 - preprocessor_logger - INFO - [apply_preprocess] - === INICIANDO PIPELINE COMPLETO DE PREPROCESAMIENTO ===\n",
      "2025-11-11 16:54:14,799 - preprocessor_logger - INFO - [apply_preprocess] - Columnas sociodemográficas: ['SD_1', 'SD_2', 'SD_3', 'SD_4', 'SD_5', 'SD_6', 'SD_7', 'SD_8', 'SD_9', 'SD_10', 'SD_11', 'SD_12', 'SD_13', 'SD_14', 'SD_15', 'SD_16', 'SD_17', 'SD_18', 'SD_19', 'SD_20', 'SD_21', 'SD_22', 'SD_23', 'SD_24', 'SD_25', 'SD_26', 'SD_27', 'SD_28', 'SD_29', 'SD_30', 'SD_31', 'SD_32', 'SD_33', 'SD_34', 'SD_35', 'SD_36', 'SD_37', 'SD_38', 'SD_39', 'SD_40', 'SD_41', 'SD_42', 'SD_43']\n",
      "2025-11-11 16:54:14,799 - preprocessor_logger - INFO - [apply_preprocess] - Columnas de productos: ['PD_0', 'PD_1', 'PD_2', 'PD_3', 'PD_4', 'PD_5', 'PD_6', 'PD_7', 'PD_8', 'PD_9', 'PD_10', 'PD_11', 'PD_12', 'PD_13', 'PD_14', 'PD_15', 'PD_16', 'PD_17', 'PD_18', 'PD_19', 'PD_20', 'PD_21', 'PD_22', 'PD_23', 'PD_24', 'PD_25', 'PD_26', 'PD_27', 'PD_28', 'PD_29', 'PD_30', 'PD_31', 'PD_32', 'PD_33', 'PD_34', 'PD_35', 'PD_36', 'PD_37', 'PD_38', 'PD_39', 'PD_40', 'PD_41']\n",
      "2025-11-11 16:54:14,799 - preprocessor_logger - INFO - [group_sociodemographic_cols] - Iniciando agrupación de columnas sociodemográficas: ['SD_1', 'SD_2', 'SD_3', 'SD_4', 'SD_5', 'SD_6', 'SD_7', 'SD_8', 'SD_9', 'SD_10', 'SD_11', 'SD_12', 'SD_13', 'SD_14', 'SD_15', 'SD_16', 'SD_17', 'SD_18', 'SD_19', 'SD_20', 'SD_21', 'SD_22', 'SD_23', 'SD_24', 'SD_25', 'SD_26', 'SD_27', 'SD_28', 'SD_29', 'SD_30', 'SD_31', 'SD_32', 'SD_33', 'SD_34', 'SD_35', 'SD_36', 'SD_37', 'SD_38', 'SD_39', 'SD_40', 'SD_41', 'SD_42', 'SD_43']\n",
      "2025-11-11 16:54:14,818 - preprocessor_logger - INFO - [group_sociodemographic_cols] - Agrupación completada - 1734 zonas únicas creadas\n",
      "2025-11-11 16:54:14,819 - preprocessor_logger - INFO - [_log_shape_change] - Agrupación sociodemográfica - Shape cambiado de (5821, 86) a (5821, 44) (Filas: 5821 → 5821, Columnas: 86 → 44)\n",
      "2025-11-11 16:54:14,819 - preprocessor_logger - INFO - [remove_duplicates] - Iniciando eliminación de duplicados exactos\n",
      "2025-11-11 16:54:14,825 - preprocessor_logger - INFO - [remove_duplicates] - Duplicados eliminados: 602 filas removidas\n",
      "2025-11-11 16:54:14,825 - preprocessor_logger - INFO - [_log_shape_change] - Eliminación de duplicados exactos - Shape cambiado de (5821, 44) a (5219, 44) (Filas: 5821 → 5219, Columnas: 44 → 44)\n",
      "2025-11-11 16:54:14,826 - preprocessor_logger - INFO - [handle_complex_duplicates] - Iniciando manejo de duplicados complejos\n",
      "2025-11-11 16:54:14,889 - preprocessor_logger - INFO - [handle_complex_duplicates] - Manejo de duplicados complejos completado - 49 grupos procesados, 98 filas eliminadas\n",
      "2025-11-11 16:54:14,889 - preprocessor_logger - INFO - [_log_shape_change] - Manejo de duplicados complejos - Shape cambiado de (5219, 44) a (5121, 44) (Filas: 5219 → 5121, Columnas: 44 → 44)\n",
      "2025-11-11 16:54:14,893 - preprocessor_logger - INFO - [get_correlations] - Calculando correlaciones con el target\n",
      "2025-11-11 16:54:14,903 - preprocessor_logger - INFO - [get_correlations] - Correlaciones calculadas - Rango: [-0.069, 0.158], Top 3: {'PD_3': 0.1583247262534565, 'PD_24': 0.15317100674424192, 'PD_38': 0.1164514538078758}\n",
      "2025-11-11 16:54:14,903 - preprocessor_logger - INFO - [get_high_pair_correlations] - Buscando correlaciones altas entre 42 columnas de productos\n",
      "2025-11-11 16:54:14,923 - preprocessor_logger - INFO - [get_high_pair_correlations] - Análisis de correlación completado - 21 pares con correlación > 0.7, 6 pares con correlación > 0.95\n",
      "2025-11-11 16:54:14,923 - preprocessor_logger - INFO - [drop_high_correlated_cols] - Iniciando eliminación de columnas altamente correlacionadas\n",
      "2025-11-11 16:54:14,924 - preprocessor_logger - INFO - [drop_high_correlated_cols] - Eliminación de columnas correlacionadas completada - 6 columnas eliminadas: ['PD_20', 'PD_10', 'PD_34', 'PD_23', 'PD_28', 'PD_21']\n",
      "2025-11-11 16:54:14,924 - preprocessor_logger - INFO - [_log_shape_change] - Eliminación de columnas correlacionadas - Shape cambiado de (5121, 44) a (5121, 38) (Filas: 5121 → 5121, Columnas: 44 → 38)\n",
      "2025-11-11 16:54:14,924 - preprocessor_logger - INFO - [correct_skewness] - Iniciando corrección de asimetría\n",
      "2025-11-11 16:54:14,928 - preprocessor_logger - INFO - [correct_skewness] - 33 columnas con asimetría > 0.5: ['PD_30', 'PD_17', 'PD_2', 'PD_19', 'PD_40', 'PD_6', 'PD_7', 'PD_36', 'PD_41', 'PD_25', 'PD_1', 'PD_31', 'PD_12', 'PD_39', 'PD_18', 'PD_32', 'PD_9', 'PD_14', 'PD_4', 'PD_38', 'PD_8', 'PD_33', 'PD_37', 'PD_5', 'PD_24', 'PD_27', 'PD_29', 'PD_11', 'PD_26', 'PD_22', 'PD_16', 'PD_35', 'PD_13']\n",
      "2025-11-11 16:54:15,045 - preprocessor_logger - INFO - [correct_skewness] - Corrección de asimetría completada - Asimetría promedio: 12.940 → 11.366\n",
      "2025-11-11 16:54:15,045 - preprocessor_logger - INFO - [apply_one_hot] - Aplicando one-hot encoding a la columna 'zone'\n",
      "2025-11-11 16:54:15,066 - preprocessor_logger - INFO - [apply_one_hot] - One-hot encoding completado - 1733 columnas zone creadas\n",
      "2025-11-11 16:54:15,067 - preprocessor_logger - INFO - [_log_shape_change] - One-hot encoding - Shape cambiado de (5121, 38) a (5121, 1770) (Filas: 5121 → 5121, Columnas: 38 → 1770)\n",
      "2025-11-11 16:54:15,067 - preprocessor_logger - INFO - [apply_preprocess] - === PIPELINE COMPLETADO EXITOSAMENTE ===\n",
      "2025-11-11 16:54:15,067 - preprocessor_logger - INFO - [apply_preprocess] - Shape final del DataFrame: (5121, 1770)\n",
      "2025-11-11 16:54:15,067 - preprocessor_logger - INFO - [apply_preprocess] - Columnas eliminadas: 6\n",
      "2025-11-11 16:54:15,068 - preprocessor_logger - INFO - [apply_preprocess] - Columnas transformadas: 33\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(df)\n",
    "processed = preprocessor.apply_preprocess(sociodemographic_cols, product_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a215c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14cc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target\"\n",
    "X = processed.drop(columns=[target])\n",
    "y = processed[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Para la regresión logistica\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d720ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisfernandocaporalmontesdeoca/miniconda3/envs/MNA_MLOps_Equipo12_backup/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/11 17:11:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/11 17:11:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LogisticRegression registrado en MLflow con métricas:\n",
      "   accuracy: 0.9288\n",
      "   precision: 0.2000\n",
      "   recall: 0.0862\n",
      "   f1_score: 0.1205\n",
      "   roc_auc: 0.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 17:11:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/11 17:11:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest registrado en MLflow con métricas:\n",
      "   accuracy: 0.9434\n",
      "   precision: 0.0000\n",
      "   recall: 0.0000\n",
      "   f1_score: 0.0000\n",
      "   roc_auc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisfernandocaporalmontesdeoca/miniconda3/envs/MNA_MLOps_Equipo12_backup/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [17:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/11/11 17:11:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/11 17:11:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost registrado en MLflow con métricas:\n",
      "   accuracy: 0.9434\n",
      "   precision: 0.5000\n",
      "   recall: 0.0172\n",
      "   f1_score: 0.0333\n",
      "   roc_auc: 0.7498\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_and_log_model(model, model_name, X_train, X_test, y_train, y_test, params=None):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "        \n",
    "        # Entrenamiento\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Métricas\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        }\n",
    "        if y_proba is not None:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log del modelo\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"✅ {model_name} registrado en MLflow con métricas:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# Entrenamiento de modelos\n",
    "# ============================================\n",
    "\n",
    "# --- Regresión Logística ---\n",
    "log_reg_params = {\"solver\": \"liblinear\", \"random_state\": 42}\n",
    "log_reg = LogisticRegression(**log_reg_params)\n",
    "train_and_log_model(log_reg, \"LogisticRegression\", X_train_scaled, X_test_scaled, y_train, y_test, log_reg_params)\n",
    "\n",
    "# --- Random Forest ---\n",
    "rf_params = {\"n_estimators\": 200, \"max_depth\": 10, \"random_state\": 42}\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "train_and_log_model(rf, \"RandomForest\", X_train, X_test, y_train, y_test, rf_params)\n",
    "\n",
    "# --- XGBoost ---\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"use_label_encoder\": False,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "}\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "train_and_log_model(xgb, \"XGBoost\", X_train, X_test, y_train, y_test, xgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ecf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
